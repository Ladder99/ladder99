#!/bin/bash

usage="
Usage: ./restore SETUP FILE

Restore a backup made with pg_dumpall.

WARNING: this deletes all data in the current databases!

Example
    ./restore oxbox postgres-2022-02-26T12-43-55.tar.gz
"

# show help
if [ $# -eq 0 ]; then
  echo "$usage"
  exit 1
fi

# get setup folder
# if starts with '.' treat as a setup folder, else a client name
if [[ $1 = .* ]]; then
    SETUP=$1
else
    SETUP=../client-$1
fi
shift

# set envars
SERVICE=postgres
# DATABASE=postgres
USER=postgres
FILE=$1 # eg backup_2022-02-26_122455.bak
# FOLDER1=$SETUP/volumes/$SERVICE/backups # eg ../client-oxbox/volumes/postgres/backups
# FOLDER2=/var/lib/postgresql/backups # docker volume path - defined in compose.yaml

# exit if no file
if [ ! -e "$FILE" ]; then
  echo File $FILE does not exist.
  exit 1
fi  

# move file into docker volume/folder temporarily
echo Moving $FILE into Docker volume $FOLDER1...
sudo mv $FILE $FOLDER1/

# import the database
echo Importing $FILE into $DATABASE...
# docker exec runs a command within a container $SERVICE.
# -i hooks up input
# psql runs commands on a postgres db
# note: you cannot mix SQL and psql meta-commands within a -c option -
# the best way is to use separate -c commands. 
# tried using a \\ separator in a heredoc but didn't work.
#   -c is a command
#   \c chooses a database
#   \! runs a shell command
# pg_restore imports a database
#   -Fc is custom format (a compressed archive made by pg_dump)
#   --clean drops all objects before restoring them
# note: need the pre and post restore functions, or else when you try to 
# write to a hypertable you'll get an error.
# see https://docs.timescale.com/timescaledb/latest/how-to-guides/backup-and-restore/pg-dump-and-restore
docker exec -i $SERVICE psql -U $USER -d $DATABASE --echo-queries \
  -c "\c $DATABASE" \
  -c "CREATE EXTENSION IF NOT EXISTS timescaledb;" \
  -c "SELECT timescaledb_pre_restore();" \
  -c "\! pg_restore -Fc --clean --verbose -d $DATABASE -U $USER $FOLDER2/$FILE" \
  -c "SELECT timescaledb_post_restore();"

# put backup file back
echo Moving $FILE back to original location...
sudo mv $FOLDER1/$FILE $FILE

echo Done.




# Step 1: Define the parameters for the Space you want to upload to.
SPACE="ladder99" # Find your endpoint in the control panel, under Settings.
REGION="nyc3" # Must be "us-east-1" when creating new Spaces. Otherwise, use the region in your endpoint (e.g. nyc3).
STORAGETYPE="STANDARD" # Storage type, can be STANDARD, REDUCED_REDUNDANCY, etc.
KEY="4OAL6WDUOREIRHW2E5VA" # Access key pair. You can create access key pairs using the control panel or API.
SECRET="$SECRET" # Secret access key defined through an environment variable.

# Step 2: Define a function that uploads your object via cURL.
function putS3
{
  path="." # The local path to the file you want to upload.
  file="hello-world.txt" # The file you want to upload.
  space_path="/" # The path within your Space where you want to upload the new file.
  space="${SPACE}"
  date=$(date +"%a, %d %b %Y %T %z")
  acl="x-amz-acl:private" # Defines Access-control List (ACL) permissions, such as private or public.
  content_type="text/plain" # Defines the type of content you are uploading.
  storage_type="x-amz-storage-class:${STORAGETYPE}"
  string="PUT\n\n$content_type\n$date\n$acl\n$storage_type\n/$space$space_path$file"
  signature=$(echo -en "${string}" | openssl sha1 -hmac "${SECRET}" -binary | base64)
  curl -s -X PUT -T "$path/$file" \ # The cURL command that uploads your file.
    -H "Host: $space.${REGION}.digitaloceanspaces.com" \
    -H "Date: $date" \
    -H "Content-Type: $content_type" \
    -H "$storage_type" \
    -H "$acl" \
    -H "Authorization: AWS ${KEY}:$signature" \
    "https://$space.${REGION}.digitaloceanspaces.com$space_path$file"
}

# Step 3: Run the putS3 function.
for file in "$path"/*; do
  putS3 "$path" "${file##*/}" "nyc-tutorial-space/"
done